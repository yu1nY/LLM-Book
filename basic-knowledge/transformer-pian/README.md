# Transformer篇

> paper：[https://arxiv.org/pdf/1706.03762](https://arxiv.org/pdf/1706.03762) （Attention Is All You Need）
>
> 这是一篇很经典的论文，网上也有很多关于该论文的解读，在这里的话，更多的是从大模型的角度来看transformer，而非传统nlp的角度，笔者本人本来非nlp方向，在大模型概念爆火之际来到该方向，中间踩了很多坑，总结此文，希望能够让读者真正有所收获。

